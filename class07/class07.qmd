---
title: "Class 7: Machine Learning 1"
author: "Jacob Hizon A17776679"
format: pdf
---

## Background 

Today we will begin our exploration of important machine learning methods with a focus on **clustering** and **dimensionality reduction**. 


To start testing these methods let's make up some sample data to cluster where we know what the answer should be.

```{r}
hist( rnorm(3000, mean = 10, sd = ) ) 
```

> Q. Can you generate 30 numbers centered at +3  and 30 numbers at -3 taken at random from a normal distribution?

```{r}
tmp <- c(rnorm(30, mean = 3), 
         rnorm(30, mean = -3) )
x <- cbind(x = tmp, y = rev(tmp))

plot(x)
```

## K-means clustering 

The main function in "base R" for K-means clustering is called `kmeans()`, let's try it out:

```{r}
k <- kmeans(x, centers = 2)
k
```

> Q. What component of your kmeans result object has the cluster centers?

```{r}
k$centers
```
> Q. What component of your kmeans result object has the cluster size (i.e. the main clustering result: which points are in which cluster)?

```{r}
k$size
```


> Q. What component of your kmeans result object has the cluster membership vector (i.e. the main clustering result: which points are in which cluster)?

```{r}
k$cluster
```

> Q. Plot the results of clustering (i.e. our data colored by the clustering result) along with the cluster centers. 

```{r}
plot(x, col = k$cluster)
points(k$centers, col = "blue", pch = 15, cex = 2)
```

> Q. Can you run `kmeans()` again and cluster `x` into 4 centers and plot the results just like we did above with coloring by cluster and the cluster centers shown in blue? 

```{r}
k4 <- kmeans(x, centers = 4)
k4
plot(x, col = k$cluster)
points(k$centers, col = "blue", pch = 15, cex = 2)
```

> **Key-point:** Kmeans will always return the clustering that we ask for (this is the "K" or "centers" in K-means)!

```{r}
k$tot.withinss
```


## Hierarchial clustering 


The main function Hierarchical clustering in base R is called `hclust()`.

One of the main differences with respect to the `kmeans()` function is that you cannot just pass your input data directly to `hclust()` - it needs a "distance matrix" as input. We can get this from lot's of places including the `dist()` function. 

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```

We can "cut" the dendrogram or tree at a given height to yield our "clusters". For this we use the function `cutree()`

```{r}
plot(hc)
abline( h =10, col = "red")
grps <- cutree(hc, h = 10)
```
```{r}
grps
```


> Q. Plot our data `x` colored by the clusterinv result from `hclust()` and `cutree()`? 

```{r}
grps <- cutree(hc, h = 10)
plot(x, col = grps)

```

```{r}
plot(hc)
abline( h =4.2, col = "red")
grps <- cutree(hc, h = 4.2)
```


## Principal Component Analysis (PCA)

PCA is a popular dimmensionality reduction technique that is widely used in bioinformatics. 


## PCA of UK food data

#Read in the data 
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

It looks like the row names are not set up properly. We can fix this 

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
x
```

A better way to do this is fix row names assignment at import time: 

```{r}
x <- read.csv(url, row.names = 1)
x
```
> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I like making the change when you first read in the csv. 

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

```

> Q3: Changing what optional argument in the above barplot() function results in the following plot?

By making the argument `beside=F`
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

### Pairs plots and heatmaps

> Q4 is missing 

> Q5: We can use the `pairs()` function to generate all pairwise plots for our countries. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)

```
It shows a pairwise comparison between each country/territory with each territory in their own respective horizontal x axis. Similarity in consumption would lie perfectly on the diagonal and any spatial deviation represents dissimilarity in consumption in a specific food category. 


## Heatmap

We can install the `pheatmap` package with the `install.packages()` command that we used previosly. Remember that we always run this in the console and not a code chunk in the quarto document. 

```{r}
library(pheatmap)

pheatmap( as.matrix(x) )
```

Of all these plots really only the `pairs()` plot was useful. This however took a bit of work to interpret and will of scale when I am looking at much bigger data sets. 

> Q6. Based on the pairs and heatmap figures, which countries cluster together and what does this suggest about their food consumption patterns? Can you easily tell what the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

Consumption patterns are roughly the same visually. No we cannot easily see the difference it looks less quantified. 


## PCA to the recue

The main function in "base R" for PCA is called `prcomp()`. 

```{r}
pca <- prcomp(t(x)) 
summary(pca)
```
> Q. How much varance is captured in the first PC?

67.44% 

> Q. How many PCs do I need to capture at the least 90% of the total varance in the dataset? 

2 PCs capture 96.5% of the total variance. 

> Q Plot our main PCA result. Folks can call this different things depending on their field of study e.g. "PC plot", or "ordination plot", "score plot", "PC1 vs PC 2 plot"...



```{r}
attributes(pca)
```

To generate a PCA score plot we want the `pca$x` component of the result object

```{r}
pca$x
```

```{r}
plot(pca$x[,1], pca$x[,2])
```
```{r}

my_cols <- c("orange", "red", "blue", "darkgreen")
```

```{r}
library(ggplot2)


ggplot(pca$x) +
  aes(x = PC1, y = PC2, label = rownames(pca$x)) +
  geom_point(size = 3, color = my_cols) +
  geom_text(vjust = -0.5) +
  xlim(-270, 500) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw()
```
 
```{r}
my_cols <- c("orange", "red", "blue", "darkgreen")

```


## Digging deeper (variable loadings)

How do the original variables (i.e. the 17 different foods) contribute to our new PCs?

```{r}
ggplot(pca$rotation) +
  aes(x = PC1, 
      y = reorder(rownames(pca$rotation), PC1)) +
  geom_col(fill = "steelblue") +
  xlab("PC1 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))
```

 